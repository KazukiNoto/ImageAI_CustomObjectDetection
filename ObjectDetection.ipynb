{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.5.10 64-bit",
   "display_name": "Python 3.5.10 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Image AIを使ったカスタムObjectDetection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection.Custom import DetectionModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ラーニングをするための"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer のセット\n",
    "trainer = DetectionModelTrainer()\n",
    "# モデルタイプをセット\n",
    "trainer.setModelTypeAsYOLOv3()"
   ]
  },
  {
   "source": [
    "data_directoryとobject_names_arrayは適宜変更(今回はNorinoHasamiyaki)\n",
    "\n",
    "複数のオブジェクトをtrainさせる場合,\n",
    "> set object_names_array=[\"object1\", \"object2\", \"object3\",...\"objectz\"]\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating anchor boxes for training images and annotation...\nAverage IOU for 9 anchors: 0.95\nAnchor Boxes generated.\nDetection configuration saved in  NorinoHasamiyaki/json/detection_config.json\n"
     ]
    }
   ],
   "source": [
    "trainer.setDataDirectory(data_directory=\"NorinoHasamiyaki\")\n",
    "trainer.setTrainConfig(object_names_array=[\"NorinoHasamiyaki\"], batch_size=4, num_experiments=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on: \t['NorinoHasamiyaki']\n",
      "Training with Batch Size:  4\n",
      "Number of Experiments:  10\n",
      "Pre-trained Model not provided. Transfer learning not in use.\n",
      "Training will start with 3 warmup experiments\n",
      "Epoch 1/13\n",
      "40/40 [==============================] - 241s 6s/step - loss: 588.7854 - yolo_layer_7_loss: 86.9859 - yolo_layer_8_loss: 161.9793 - yolo_layer_9_loss: 339.8202 - val_loss: 1720.6258 - val_yolo_layer_7_loss: 379.0680 - val_yolo_layer_8_loss: 591.0728 - val_yolo_layer_9_loss: 750.4850\n",
      "Epoch 2/13\n",
      "40/40 [==============================] - 242s 6s/step - loss: 350.0670 - yolo_layer_7_loss: 65.6956 - yolo_layer_8_loss: 101.0995 - yolo_layer_9_loss: 183.2719 - val_loss: 8489.7137 - val_yolo_layer_7_loss: 1989.0101 - val_yolo_layer_8_loss: 2742.0551 - val_yolo_layer_9_loss: 3758.6485\n",
      "Epoch 3/13\n",
      "40/40 [==============================] - 255s 6s/step - loss: 102.8128 - yolo_layer_7_loss: 17.2143 - yolo_layer_8_loss: 31.4757 - yolo_layer_9_loss: 54.1227 - val_loss: 158.2956 - val_yolo_layer_7_loss: 71.9795 - val_yolo_layer_8_loss: 55.0509 - val_yolo_layer_9_loss: 31.2653\n",
      "Epoch 4/13\n",
      "40/40 [==============================] - 254s 6s/step - loss: 36.7640 - yolo_layer_7_loss: 6.0444 - yolo_layer_8_loss: 12.0122 - yolo_layer_9_loss: 18.7074 - val_loss: 60.8221 - val_yolo_layer_7_loss: 14.0655 - val_yolo_layer_8_loss: 19.5692 - val_yolo_layer_9_loss: 27.1875\n",
      "Epoch 5/13\n",
      "40/40 [==============================] - 586s 15s/step - loss: 24.9047 - yolo_layer_7_loss: 5.9523 - yolo_layer_8_loss: 8.4404 - yolo_layer_9_loss: 10.5120 - val_loss: 37.5100 - val_yolo_layer_7_loss: 8.6391 - val_yolo_layer_8_loss: 11.9327 - val_yolo_layer_9_loss: 16.9382\n",
      "Epoch 6/13\n",
      "40/40 [==============================] - 556s 14s/step - loss: 21.1278 - yolo_layer_7_loss: 6.9962 - yolo_layer_8_loss: 7.0705 - yolo_layer_9_loss: 7.0611 - val_loss: 28.6627 - val_yolo_layer_7_loss: 8.5067 - val_yolo_layer_8_loss: 9.1965 - val_yolo_layer_9_loss: 10.9595\n",
      "Epoch 7/13\n",
      "40/40 [==============================] - 444s 11s/step - loss: 15.2031 - yolo_layer_7_loss: 4.6509 - yolo_layer_8_loss: 5.2815 - yolo_layer_9_loss: 5.2707 - val_loss: 22.4785 - val_yolo_layer_7_loss: 7.0498 - val_yolo_layer_8_loss: 8.3239 - val_yolo_layer_9_loss: 7.1049\n",
      "Epoch 8/13\n",
      "40/40 [==============================] - 478s 12s/step - loss: 12.5682 - yolo_layer_7_loss: 3.6828 - yolo_layer_8_loss: 4.8929 - yolo_layer_9_loss: 3.9924 - val_loss: 14.2352 - val_yolo_layer_7_loss: 2.1097 - val_yolo_layer_8_loss: 7.3449 - val_yolo_layer_9_loss: 4.7806\n",
      "Epoch 9/13\n",
      "40/40 [==============================] - 340s 9s/step - loss: 11.6178 - yolo_layer_7_loss: 3.7234 - yolo_layer_8_loss: 4.5384 - yolo_layer_9_loss: 3.3560 - val_loss: 14.5173 - val_yolo_layer_7_loss: 4.5898 - val_yolo_layer_8_loss: 5.9321 - val_yolo_layer_9_loss: 3.9954\n",
      "Epoch 10/13\n",
      "40/40 [==============================] - 423s 11s/step - loss: 10.4276 - yolo_layer_7_loss: 3.4014 - yolo_layer_8_loss: 4.5473 - yolo_layer_9_loss: 2.4789 - val_loss: 9.8170 - val_yolo_layer_7_loss: 1.6033 - val_yolo_layer_8_loss: 5.6388 - val_yolo_layer_9_loss: 2.5750\n",
      "Epoch 11/13\n",
      "40/40 [==============================] - 538s 13s/step - loss: 9.4386 - yolo_layer_7_loss: 3.1088 - yolo_layer_8_loss: 4.1154 - yolo_layer_9_loss: 2.2144 - val_loss: 11.0448 - val_yolo_layer_7_loss: 2.3779 - val_yolo_layer_8_loss: 5.9941 - val_yolo_layer_9_loss: 2.6729\n",
      "Epoch 12/13\n",
      "40/40 [==============================] - 479s 12s/step - loss: 9.4891 - yolo_layer_7_loss: 3.4630 - yolo_layer_8_loss: 4.1018 - yolo_layer_9_loss: 1.9243 - val_loss: 9.9503 - val_yolo_layer_7_loss: 1.9886 - val_yolo_layer_8_loss: 5.4201 - val_yolo_layer_9_loss: 2.5416\n",
      "Epoch 13/13\n",
      "40/40 [==============================] - 335s 8s/step - loss: 7.5526 - yolo_layer_7_loss: 2.1993 - yolo_layer_8_loss: 3.7860 - yolo_layer_9_loss: 1.5673 - val_loss: 8.4950 - val_yolo_layer_7_loss: 2.6859 - val_yolo_layer_8_loss: 4.1225 - val_yolo_layer_9_loss: 1.6867\n"
     ]
    }
   ],
   "source": [
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection.Custom import CustomVideoObjectDetection\n",
    "import os\n",
    "\n",
    "DETECTION_JSON=\"NorinoHasamiyaki/json/detection_config.json\"\n",
    "MODEL_PATH=\"NorinoHasamiyaki/models/detection_model-ex-008--loss-0009.107.h5\"\n",
    "MODEL_PATH=\"NorinoHasamiyaki/models/detection_model-ex-013--loss-0007.553.h5\"\n",
    "\n",
    "execution_path = os.getcwd()\n",
    "\n",
    "video_detector = CustomVideoObjectDetection()\n",
    "video_detector.setModelTypeAsYOLOv3()\n",
    "video_detector.setModelPath(MODEL_PATH)\n",
    "video_detector.setJsonPath(DETECTION_JSON)\n",
    "video_detector.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘Detect’: File exists\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Detect/TestVideo.mp4'"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# 検証結果を保存するためのvideoディレクトリを作成\n",
    "! mkdir Detect\n",
    "# videoディレクトリに検証用videoファイルをコピー\n",
    "import shutil\n",
    "shutil.copyfile(\"Anotation/ItemMov1.mp4\", \"Detect/TestVideo.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing Frame :  1\n",
      "Processing Frame :  2\n",
      "Processing Frame :  3\n",
      "Processing Frame :  4\n",
      "Processing Frame :  5\n",
      "Processing Frame :  6\n",
      "Processing Frame :  7\n",
      "Processing Frame :  8\n",
      "Processing Frame :  9\n",
      "Processing Frame :  10\n",
      "Processing Frame :  11\n",
      "Processing Frame :  12\n",
      "Processing Frame :  13\n",
      "Processing Frame :  14\n",
      "Processing Frame :  15\n",
      "Processing Frame :  16\n",
      "Processing Frame :  17\n",
      "Processing Frame :  18\n",
      "Processing Frame :  19\n",
      "Processing Frame :  20\n",
      "Processing Frame :  21\n",
      "Processing Frame :  22\n",
      "Processing Frame :  23\n",
      "Processing Frame :  24\n",
      "Processing Frame :  25\n",
      "Processing Frame :  26\n",
      "Processing Frame :  27\n",
      "Processing Frame :  28\n",
      "Processing Frame :  29\n",
      "Processing Frame :  30\n",
      "Processing Frame :  31\n",
      "Processing Frame :  32\n",
      "Processing Frame :  33\n",
      "Processing Frame :  34\n",
      "Processing Frame :  35\n",
      "Processing Frame :  36\n",
      "Processing Frame :  37\n",
      "Processing Frame :  38\n",
      "Processing Frame :  39\n",
      "Processing Frame :  40\n",
      "Processing Frame :  41\n",
      "Processing Frame :  42\n",
      "Processing Frame :  43\n",
      "Processing Frame :  44\n",
      "Processing Frame :  45\n",
      "Processing Frame :  46\n",
      "Processing Frame :  47\n",
      "Processing Frame :  48\n",
      "Processing Frame :  49\n",
      "Processing Frame :  50\n",
      "Processing Frame :  51\n",
      "Processing Frame :  52\n",
      "Processing Frame :  53\n",
      "Processing Frame :  54\n",
      "Processing Frame :  55\n",
      "Processing Frame :  56\n",
      "Processing Frame :  57\n",
      "Processing Frame :  58\n",
      "Processing Frame :  59\n",
      "Processing Frame :  60\n",
      "Processing Frame :  61\n",
      "Processing Frame :  62\n",
      "Processing Frame :  63\n",
      "Processing Frame :  64\n",
      "Processing Frame :  65\n",
      "Processing Frame :  66\n",
      "Processing Frame :  67\n",
      "Processing Frame :  68\n",
      "Processing Frame :  69\n",
      "Processing Frame :  70\n",
      "Processing Frame :  71\n",
      "Processing Frame :  72\n",
      "Processing Frame :  73\n",
      "Processing Frame :  74\n",
      "Processing Frame :  75\n",
      "Processing Frame :  76\n",
      "Processing Frame :  77\n",
      "Processing Frame :  78\n",
      "Processing Frame :  79\n",
      "Processing Frame :  80\n",
      "Processing Frame :  81\n",
      "Processing Frame :  82\n",
      "Processing Frame :  83\n",
      "Processing Frame :  84\n",
      "Processing Frame :  85\n",
      "Processing Frame :  86\n",
      "Processing Frame :  87\n",
      "Processing Frame :  88\n",
      "Processing Frame :  89\n",
      "Processing Frame :  90\n",
      "Processing Frame :  91\n",
      "Processing Frame :  92\n",
      "Processing Frame :  93\n",
      "Processing Frame :  94\n",
      "Processing Frame :  95\n",
      "Processing Frame :  96\n",
      "Processing Frame :  97\n",
      "Processing Frame :  98\n",
      "Processing Frame :  99\n",
      "Processing Frame :  100\n",
      "Processing Frame :  101\n",
      "Processing Frame :  102\n",
      "Processing Frame :  103\n",
      "Processing Frame :  104\n",
      "Processing Frame :  105\n",
      "Processing Frame :  106\n",
      "Processing Frame :  107\n",
      "Processing Frame :  108\n",
      "Processing Frame :  109\n",
      "Processing Frame :  110\n",
      "Processing Frame :  111\n",
      "Processing Frame :  112\n",
      "Processing Frame :  113\n",
      "Processing Frame :  114\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Detect/DetectVideo2.avi'"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "VIDEO_FILE = \"Detect/TestVideo2.mp4\"\n",
    "DETECT_VIDEO=\"Detect/DetectVideo2\"\n",
    "\n",
    "# 認識後、バウンディングボックスを表示した動画を作成\n",
    "# input_file_pathを読み込んで、detection後にoutput_file_pathで作成した名前のファイルが作成される\n",
    "video_detector.detectObjectsFromVideo(\n",
    "    input_file_path=VIDEO_FILE,\n",
    "    output_file_path=DETECT_VIDEO,\n",
    "    frames_per_second=40,\n",
    "    minimum_percentage_probability=12,\n",
    "    log_progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}