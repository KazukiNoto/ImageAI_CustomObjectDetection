{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.5.10 64-bit",
   "display_name": "Python 3.5.10 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Image AIを使ったカスタムObjectDetection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imageai.Detection.Custom import DetectionModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ラーニングをするための"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer のセット\n",
    "trainer = DetectionModelTrainer()\n",
    "# モデルタイプをセット\n",
    "trainer.setModelTypeAsYOLOv3()"
   ]
  },
  {
   "source": [
    "data_directoryとobject_names_arrayは適宜変更(今回はNorinoHasamiyaki)\n",
    "\n",
    "複数のオブジェクトをtrainさせる場合,\n",
    "> set object_names_array=[\"object1\", \"object2\", \"object3\",...\"objectz\"]\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating anchor boxes for training images and annotation...\nAverage IOU for 9 anchors: 0.97\nAnchor Boxes generated.\nDetection configuration saved in  NorinoHasamiyaki/json/detection_config.json\n"
     ]
    }
   ],
   "source": [
    "trainer.setDataDirectory(data_directory=\"NorinoHasamiyaki\")\n",
    "trainer.setTrainConfig(object_names_array=[\"NorinoHasamiyaki\"], batch_size=4, num_experiments=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on: \t['NorinoHasamiyaki']\n",
      "Training with Batch Size:  4\n",
      "Number of Experiments:  1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/imageai/Detection/Custom/yolo.py:24: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Pre-trained Model not provided. Transfer learning not in use.\n",
      "Training will start with 3 warmup experiments\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/4\n",
      "40/40 [==============================] - 272s 7s/step - loss: 715.9645 - yolo_layer_1_loss: 99.1634 - yolo_layer_2_loss: 217.3859 - yolo_layer_3_loss: 399.4152 - val_loss: 1256.4606 - val_yolo_layer_1_loss: 277.0042 - val_yolo_layer_2_loss: 419.2131 - val_yolo_layer_3_loss: 560.2433\n",
      "Epoch 2/4\n",
      "40/40 [==============================] - 252s 6s/step - loss: 358.9175 - yolo_layer_1_loss: 62.6153 - yolo_layer_2_loss: 104.5177 - yolo_layer_3_loss: 191.7845 - val_loss: 7904.2363 - val_yolo_layer_1_loss: 2157.9398 - val_yolo_layer_2_loss: 3323.6151 - val_yolo_layer_3_loss: 2422.6814\n",
      "Epoch 3/4\n",
      "40/40 [==============================] - 257s 6s/step - loss: 117.0231 - yolo_layer_1_loss: 18.8301 - yolo_layer_2_loss: 31.2082 - yolo_layer_3_loss: 66.9848 - val_loss: 95.6437 - val_yolo_layer_1_loss: 18.8226 - val_yolo_layer_2_loss: 19.7429 - val_yolo_layer_3_loss: 57.0782\n",
      "Epoch 4/4\n",
      "40/40 [==============================] - 235s 6s/step - loss: 33.8647 - yolo_layer_1_loss: 4.7967 - yolo_layer_2_loss: 6.4643 - yolo_layer_3_loss: 22.6037 - val_loss: 55.1250 - val_yolo_layer_1_loss: 16.9442 - val_yolo_layer_2_loss: 7.6215 - val_yolo_layer_3_loss: 30.5593\n"
     ]
    }
   ],
   "source": [
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection.Custom import CustomVideoObjectDetection\n",
    "import os\n",
    "\n",
    "DETECTION_JSON=\"NorinoHasamiyaki/json/detection_config.json\"\n",
    "MODEL_PATH=\"NorinoHasamiyaki/models/detection_model-ex-008--loss-0009.107.h5\"\n",
    "MODEL_PATH=\"NorinoHasamiyaki/models/detection_model-ex-013--loss-0007.553.h5\"\n",
    "\n",
    "execution_path = os.getcwd()\n",
    "\n",
    "video_detector = CustomVideoObjectDetection()\n",
    "video_detector.setModelTypeAsYOLOv3()\n",
    "video_detector.setModelPath(MODEL_PATH)\n",
    "video_detector.setJsonPath(DETECTION_JSON)\n",
    "video_detector.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘Detect’: File exists\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Detect/TestVideo.mp4'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# 検証結果を保存するためのvideoディレクトリを作成\n",
    "! mkdir Detect\n",
    "# videoディレクトリに検証用videoファイルをコピー\n",
    "import shutil\n",
    "shutil.copyfile(\"Anotation/ItemMov1.mp4\", \"Detect/TestVideo.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing Frame :  1\n",
      "Processing Frame :  2\n",
      "Processing Frame :  3\n",
      "Processing Frame :  4\n",
      "Processing Frame :  5\n",
      "Processing Frame :  6\n",
      "Processing Frame :  7\n",
      "Processing Frame :  8\n",
      "Processing Frame :  9\n",
      "Processing Frame :  10\n",
      "Processing Frame :  11\n",
      "Processing Frame :  12\n",
      "Processing Frame :  13\n",
      "Processing Frame :  14\n",
      "Processing Frame :  15\n",
      "Processing Frame :  16\n",
      "Processing Frame :  17\n",
      "Processing Frame :  18\n",
      "Processing Frame :  19\n",
      "Processing Frame :  20\n",
      "Processing Frame :  21\n",
      "Processing Frame :  22\n",
      "Processing Frame :  23\n",
      "Processing Frame :  24\n",
      "Processing Frame :  25\n",
      "Processing Frame :  26\n",
      "Processing Frame :  27\n",
      "Processing Frame :  28\n",
      "Processing Frame :  29\n",
      "Processing Frame :  30\n",
      "Processing Frame :  31\n",
      "Processing Frame :  32\n",
      "Processing Frame :  33\n",
      "Processing Frame :  34\n",
      "Processing Frame :  35\n",
      "Processing Frame :  36\n",
      "Processing Frame :  37\n",
      "Processing Frame :  38\n",
      "Processing Frame :  39\n",
      "Processing Frame :  40\n",
      "Processing Frame :  41\n",
      "Processing Frame :  42\n",
      "Processing Frame :  43\n",
      "Processing Frame :  44\n",
      "Processing Frame :  45\n",
      "Processing Frame :  46\n",
      "Processing Frame :  47\n",
      "Processing Frame :  48\n",
      "Processing Frame :  49\n",
      "Processing Frame :  50\n",
      "Processing Frame :  51\n",
      "Processing Frame :  52\n",
      "Processing Frame :  53\n",
      "Processing Frame :  54\n",
      "Processing Frame :  55\n",
      "Processing Frame :  56\n",
      "Processing Frame :  57\n",
      "Processing Frame :  58\n",
      "Processing Frame :  59\n",
      "Processing Frame :  60\n",
      "Processing Frame :  61\n",
      "Processing Frame :  62\n",
      "Processing Frame :  63\n",
      "Processing Frame :  64\n",
      "Processing Frame :  65\n",
      "Processing Frame :  66\n",
      "Processing Frame :  67\n",
      "Processing Frame :  68\n",
      "Processing Frame :  69\n",
      "Processing Frame :  70\n",
      "Processing Frame :  71\n",
      "Processing Frame :  72\n",
      "Processing Frame :  73\n",
      "Processing Frame :  74\n",
      "Processing Frame :  75\n",
      "Processing Frame :  76\n",
      "Processing Frame :  77\n",
      "Processing Frame :  78\n",
      "Processing Frame :  79\n",
      "Processing Frame :  80\n",
      "Processing Frame :  81\n",
      "Processing Frame :  82\n",
      "Processing Frame :  83\n",
      "Processing Frame :  84\n",
      "Processing Frame :  85\n",
      "Processing Frame :  86\n",
      "Processing Frame :  87\n",
      "Processing Frame :  88\n",
      "Processing Frame :  89\n",
      "Processing Frame :  90\n",
      "Processing Frame :  91\n",
      "Processing Frame :  92\n",
      "Processing Frame :  93\n",
      "Processing Frame :  94\n",
      "Processing Frame :  95\n",
      "Processing Frame :  96\n",
      "Processing Frame :  97\n",
      "Processing Frame :  98\n",
      "Processing Frame :  99\n",
      "Processing Frame :  100\n",
      "Processing Frame :  101\n",
      "Processing Frame :  102\n",
      "Processing Frame :  103\n",
      "Processing Frame :  104\n",
      "Processing Frame :  105\n",
      "Processing Frame :  106\n",
      "Processing Frame :  107\n",
      "Processing Frame :  108\n",
      "Processing Frame :  109\n",
      "Processing Frame :  110\n",
      "Processing Frame :  111\n",
      "Processing Frame :  112\n",
      "Processing Frame :  113\n",
      "Processing Frame :  114\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Detect/DetectVideo2.avi'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "VIDEO_FILE = \"Detect/TestVideo2.mp4\"\n",
    "DETECT_VIDEO=\"Detect/DetectVideo2\"\n",
    "\n",
    "# 認識後、バウンディングボックスを表示した動画を作成\n",
    "# input_file_pathを読み込んで、detection後にoutput_file_pathで作成した名前のファイルが作成される\n",
    "video_detector.detectObjectsFromVideo(\n",
    "    input_file_path=VIDEO_FILE,\n",
    "    output_file_path=DETECT_VIDEO,\n",
    "    frames_per_second=40,\n",
    "    minimum_percentage_probability=12,\n",
    "    log_progress=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}